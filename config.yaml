## Global settings affect all services
global:
  ## Whether to used the combined db - if this is true all the databases used
  ## by magda will be consolidated into a single pod. If it's false, they'll
  ## all run in separate pods unless some managed postgres solution (e.g.
  ## useCloudSql) is specified elsewhere.
  ## Running in separate pods scales better, but requires more resources
  useCombinedDb: false

  ## Whether to use the Google Cloud SQL managed postgres solution. Only set
  ## this to true if you already have cloud sql set up and be sure to look at
  ## settings for the cloud-sql-proxy service below.
  useCloudSql: true

  ## The external URL of the installation, e.g. "https://search.data.gov.au".
  ## This is used in a number of places, including linking back to the site
  ## from emails, creating a absolute sitemap URL for robots.txt, etc.
  externalUrl: https://data.gov.au

  ## How to handle rolling updates during a deployment. If set to 0, kubernetes
  ## will happily take down pods while it replaces them (useful for dev setups).
  ## If set to 1, kubernetes will make sure one pod is available to take requests
  ## during upgrades. 0 by default.
  rollingUpdate:
    maxUnavailable: 1

  image:
    ## What tag of the magda images to get. Look at
    ## for versions. https://github.com/magda-io/magda/releases/latest.
    ## By default this is the same version as the helm chart.
    # tag:
    ## The docker repository to get the images from - defaults to the official
    ## data61 docker hub repo
    # repository: data61
    ## The imagePullPolicy to use for images - generally unless you're actively
    ## trying to track development this should be "IfNotPresent", otherwise if
    ## you are it can be "Always". Defaults to "IfNotPresent"
    pullPolicy: IfNotPresent
    ## Kubernetes secret to use if you're fetching images from a private repository
    # imagePullSecret: ""

  ## Whether to expose the node ports of the pods in the cluster - useful for
  ## development, because it allows you to connect directly to pods without using
  ## kubectl proxy
  # exposeNodePorts: false

  ## Whether to enable kubernetes liveness probes - these continually hit pods in
  ## order to see whether they're still alive and restart them upon too many failures.
  ## This is essential for prod, but tends to create a lot of overhead in development
  enableLivenessProbes: true

  ## The admin to use as the root admin user in the database. Defaults to the very first
  ## uuidv4
  # defaultAdminUserId: "00000000-0000-4000-8000-000000000000"

  ## Default log level to use across the project. Currently this only affects scala-based
  ## services
  # logLevel: INFO

  ## Whether to disable database authentication - useful in development, a very very bad
  ## idea in production
  # noDbAuth: false
  
  ## Whether to give the pods priority classes - these ensure that essential pods like
  ## ElasticSearch, Databases and the Gateway are schedules ahead of less essential pods
  ## like minions, but tend to cause problems if you're running Magda over multiple
  ## namespaces. Make sure that the priorities tag is true if you turn this on.
  enablePriorityClass: true

  ## Specify the shared common settings for all connectors
  connectors:
    ## Whether to run an initial job to crawl all these on install
    includeInitialJobs: false

    ## Whether to run recurring jobs on the schedule specified per connector.
    includeCronJobs: true

  ## Settings for configuring Open Functions as a Service, which is used for some inconsistent
  ## workloads in Magda
  openfaas:
    ## Turn on / off openfaas
    enabled: false
    ## Only allow someone who's logged into magda as an admin to access the openfaas gateway
    allowAdminOnly: true

## Tags allow for services to be turned on and off on the fly... e.g. if you don't need
## to send emails, turn the correspondence-api off
tags:
  ## "all" is a shortcut which turns on all services except minions.
  all: false

  ## General services. Note that if you are using combined-db, you need to set both
  ## combined-db and the databases that you are using (e.g. authorization-db) to true.
  ## You can find explanations of what each of these are below

  admin-api: true
  apidocs-server: true
  authorization-api: true
  authorization-db: true
  cloud-sql-proxy: true
  combined-db: true
  content-db: true
  content-api: true
  correspondence-api: true
  elasticsearch: true
  gateway: true
  indexer: true
  preview-map: true
  registry-api: true
  registry-db: true
  search-api: true
  session-db: true
  web-server: true
  connectors: true
  opa: true

  ## Whether to use an ingress, which is necessary for HTTPS and using the Google CDN with
  ## Google Kubernetes Engine. By default this is off, and access is provided to the cluster
  ## through a Kubernetes LoadBalancer service
  ## Magda's built-in ingress chart certificate object api version is a bit old for newer version cert manager
  ## We included a forked version in this repo
  ingress: false

  ## Priority classes - see explanation under "enablePriorityClasses" above
  priorities: true

  ## Minions, which listen to changes in the registry and add extra metadata
  minion-broken-link: true
  minion-linked-data-rating: true
  minion-visualization: true
  minion-format: true

  ## Tenant api / db (pre-alpha)
  tenant-api: false
  tenant-db: false

  ## storage-api is used to store dataset data files. Also deploys minio to store them
  storage-api: false

  ## url processors are used in the add dataset flow to allow metadata to be scraped from data portals and APIs
  url-processors: false

  minion-ckan-exporter: false

## These are the external jobs that pull data in from other data portals. A default is provided here for you.
connector-dga:
  config:
    ## Unique id to identify this connector and records that are harvested from it
    id: dga
    ## Friendly readable name
    name: "data.gov.au"
    ## The base URL of the place to source data from
    sourceUrl: "https://data.gov.au/data/"
    ## When crawling through from beginning to end, how big should the individual requests be in records?
    pageSize: 100
    ## Crontab schedule for how often this should happen.
    schedule: "0 * * * *"
    ## CKAN-specific config: what harvest sources to ignore. * will ignore everything that's been harvested
    ## from another portal
    ignoreHarvestSources: ["*"]
  
connector-act:
  config:
    id: act
    name: ACT Government data.act.gov.au
    sourceUrl: https://www.data.act.gov.au/data.json
    schedule: 0 0 */3 * *

connector-actmapi:
  config:
    id: actmapi
    name: ACT Government ACTMAPi
    sourceUrl: https://actmapi-actgov.opendata.arcgis.com/data.json
    schedule: 45 0 */3 * *

connector-aims:
  config:
    id: aims
    name: Australian Institute of Marine Science
    sourceUrl: https://geo.aims.gov.au/geonetwork/srv/eng/csw
    pageSize: 100
    schedule: 0 1 */3 * *

connector-aodn:
  config:
    id: aodn
    name: Australian Oceans Data Network
    sourceUrl: https://catalogue.aodn.org.au/geonetwork/srv/eng/csw
    pageSize: 100
    schedule: 30 1 */3 * *

connector-bom:
  config:
    id: bom
    name: Bureau of Meteorology
    sourceUrl: http://www.bom.gov.au/geonetwork/srv/eng/csw
    pageSize: 100
    schedule: 0 13 */3 * *

connector-aurin:
  config:
    id: aurin
    name: Australian Urban Research Infrastructure Network
    sourceUrl: https://openapi.aurin.org.au/public/csw
    pageSize: 100
    schedule: 0 1 */3 * *

connector-brisbane:
  config:
    id: brisbane
    name: Brisbane City Council
    sourceUrl: https://www.data.brisbane.qld.gov.au/data/
    pageSize: 100
    schedule: 10 * */1 * *

connector-hobart:
  config:
    id: hobart
    name: City of Hobart Open Data Portal
    sourceUrl: https://data-1-hobartcc.opendata.arcgis.com/data.json
    schedule: 30 2 */3 * *

connector-marlin:
  config:
    id: marlin
    name: CSIRO Marlin
    sourceUrl: http://www.marlin.csiro.au/geonetwork/srv/eng/csw
    pageSize: 50
    schedule: 30 3 */3 * *

connector-environment:
  config:
    id: environment
    name: Department of the Environment and Energy
    sourceUrl: http://www.environment.gov.au/fed/csw
    pageSize: 100
    schedule: 30 13 */3 * *

connector-esta:
  config:
    id: esta
    name: ESTA Open Data
    sourceUrl: http://data-esta000.opendata.arcgis.com/data.json
    schedule: 30 4 */3 * *

connector-ga:
  config:
    id: ga
    name: Geoscience Australia
    sourceUrl: https://ecat.ga.gov.au/geonetwork/srv/eng/csw
    outputSchema: http://standards.iso.org/iso/19115/-3/mdb/1.0
    pageSize: 100
    schedule: 0 5 */3 * *

connector-logan:
  config:
    id: logan
    name: Logan City Council
    sourceUrl: https://data-logancity.opendata.arcgis.com/data.json
    schedule: 30 5 */3 * *

connector-melbourne:
  config:
    id: melbourne
    name: Melbourne Data
    sourceUrl: https://data.melbourne.vic.gov.au/data.json
    schedule: 0 6 */3 * *

connector-mrt:
  config:
    id: mrt
    name: Mineral Resources Tasmania
    sourceUrl: http://www.mrt.tas.gov.au/web-catalogue/srv/eng/csw
    pageSize: 100
    schedule: 30 6 */3 * *

connector-moretonbay:
  config:
    id: moretonbay
    name: Moreton Bay Regional Council Data Portal
    sourceUrl: http://datahub.moretonbay.qld.gov.au/data.json
    schedule: 0 7 */3 * *

connector-neii:
  config:
    id: neii
    name: National Environmental Information Infrastructure
    sourceUrl: http://neii.bom.gov.au/services/catalogue/csw
    pageSize: 100
    schedule: 30 7 */3 * *

connector-nsw:
  config:
    id: nsw
    name: New South Wales Government
    sourceUrl: https://data.nsw.gov.au/data/
    pageSize: 100
    schedule: 20 * * * *

connector-sdinsw:
  config:
    id: sdinsw
    name: NSW Land and Property
    sourceUrl: https://sdi.nsw.gov.au/csw
    pageSize: 100
    schedule: 0 9 */3 * *

connector-qld:
  config:
    id: qld
    name: Queensland Government
    sourceUrl: https://data.qld.gov.au/
    pageSize: 100
    schedule: 30 * * * *

connector-sa:
  config:
    id: sa
    name: South Australia Government
    sourceUrl: https://data.sa.gov.au/data/
    pageSize: 100
    schedule: 40 * * * *

connector-listtas:
  config:
    id: listtas
    name: Tasmania TheList
    sourceUrl: https://data.thelist.tas.gov.au:443/datagn/srv/eng/csw
    pageSize: 100
    schedule: 30 10 */3 * *

connector-tern:
  config:
    id: tern
    name: Terrestrial Ecosystem Research Network
    sourceUrl: http://data.auscover.org.au/geonetwork/srv/eng/csw
    pageSize: 100
    schedule: 0 11 */3 * *

connector-vic:
  config:
    id: vic
    name: Victoria Government
    sourceUrl: https://discover.data.vic.gov.au/
    pageSize: 100
    schedule: 50 * * * *
    ignoreHarvestSources: []
    presetRecordAspects:
    - id: organization-details
      recordType: Organization
      data: 
        title: "Victoria Government"
        jurisdiction: "Victoria Government"


connector-wa:
  config:
    id: wa
    name: Western Australia Government
    sourceUrl: https://catalogue.data.wa.gov.au/
    pageSize: 100
    schedule: 55 * * * *

connector-dap:
  config:
    id: dap
    name: CSIRO
    sourceUrl: https://data.csiro.au/dap/ws/v2/
    pageSize: 100
    schedule: 0 14 * * 6

connector-vic-cardinia:
  config:
    id: vic-cardinia
    name: Cardinia Shire Council
    sourceUrl: https://data-cscgis.opendata.arcgis.com/data.json
    schedule: 30 11 * * *

connector-nt-darwin:
  config:
    id: nt-darwin
    name: City of Darwin
    sourceUrl: https://open-darwin.opendata.arcgis.com/data.json
    schedule: 0 12 */3 * *

connector-southern-grampians:
  config:
    id: southern-grampians
    name: Southern Grampians Shire Council
    sourceUrl: https://www.connectgh.com.au/data.json
    schedule: 30 12 */3 * *

connector-gbrmpa:
  config:
    id: gbrmpa
    name: GBRMPA Geoportal
    sourceUrl: https://geoportal.gbrmpa.gov.au/data.json
    schedule: 30 15 */3 * *

connector-launceston:
  config:
    id: launceston
    name: City of Launceston Open Data
    sourceUrl: https://opendata.launceston.tas.gov.au/data.json
    schedule: 0 3 */3 * *

  
      
      
      ## CKAN-only: Only get datasets that match this organisation name - you can only have one due to limitations
      ## in the CKAN API, if you need multiple organisations make multiple connectors.
      #allowedOrganisationName: "doee"

      ## Extra data that will be added to the 'source' aspect
      # extras:
      #   a: 122
      #   b:
      #     - 1

      ## Extra data that will be merged into the aspects that are recieved from the connector
      # presetRecordAspects:
      #   - id: "dcat-dataset-strings"
      #     recordType: "dataset"
      #     data:
      #       creation:
      #         isOpenData: true

## Settings for the ingress. You need one of these if you want to do SSL or use the Google CDN on GKE
## Otherwise using the built-in Kubernetes LoadBalancer works ok.
ingress:
  ## The hostname of the site
  hostname: search.data.gov.au
  ## The name of the external ip to use - used when using the GCE ingress type
  ipName: search-data-gov-au-ingress
  ## The class of the ingress - if running on GKE, gce will automatically work, otherwise you
  ## can use "nginx" which requires extra setup
  ingressClass: gce
  ## Whether to enable TLS - requires a TLS secret to be set:
  ## https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/
  enableTls: true
  ## Name of the tls secret, defaults to "magda-cert-tls"
  tlsSecretName: "magda-cert-tls"
  ## Domains to attempt to obtain an SSL certificate for from let's encrypt, if using certmanager
  ## (requires extra setup)
  domains:
    - search.data.gov.au
    - letest.search.data.gov.au

# All core components are configurable under magda / magda-core instead
# connectors & minions are not core components anymore
# - built-in minions are parts of `magda` charts
# - no connectors are included in either `magda` or `magda-core` chart. Users should add them to their own deply chart e.g. [./chart](./chart) if needed.
magda:

  ## Minions are part of magda chart. Thus, should be configured under `magda:`
  ## Settings for the broken link minion, which crawls URLs of distributions in order
  ## to determine whether they're still available.
  ## Settings for the broken link minion, which crawls URLs of distributions in order
  ## to determine whether they're still available.
  minion-broken-link:
    schedule: "0 0 */7 * *"
    resources:
      requests:
        cpu: 50m
        memory: 250Mi
      limits:
        cpu: 100m
        memory: 1000Mi
    domainWaitTimeConfig:
      data.csiro.au: 5
      data.gov.au: 5
      data.act.gov.au: 30
    
  ## Settings for the format minion, which tries to determine the true format of a 
  ## distribution based on a number of criteria.
  minion-format:
    resources:
      requests:
        cpu: 20m
        memory: 100Mi
      limits:
        cpu: 50m
        memory: 500Mi

  ## Settings for the linked data rating minion, which determines a rating for a dataset
  ## based off the availability, license and format of a dataset
  minion-linked-data-rating:
    resources:
      requests:
        cpu: 10m
        memory: 50Mi
      limits:
        cpu: 50m
        memory: 500Mi

  ## Settings for the visualization minion, which looks at tabular data in order to
  ## determine which columns can be charted.
  minion-visualization:
    resources:
      requests:
        cpu: 100m
        memory: 100Mi
      limits:
        cpu: 500m
        memory: 500Mi

  magda-core:

    ## Below are individual settings for services. All services can be customised as follows:
    # servicename:
      ## The number of replicas to run
      # replicas: 1

      ## The service's image as per the global options under "global.image" like so:
      # image:
        # tag: vx.x.x
        # pullPolicy: IfNotPresent
        # imagePullSecret: ""

      ## They can also have their resource requirements customised individually as per
      ## https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/ like so:
      # resources:
        # requests:
          # cpu: 0m
          # memory: 0Mi
        # limits:
          # cpu: 0m
          # memory: 0Mi

    ## Furthermore, all the stateful sets (elasticsearch.data and anything ending in -db) can
    ## have the volume that they claim customised like so:
    # databasename-db:
      # data:
        ## The amount of storage to allocate - be generous because it's often not easy to resize
        # storage: 200Gi
        ## The storage class - needs to correspond to a storage class configured in kubernetes
        # storageClass: "ssd"

    ## Database pods (anything ending in -db) can also be configured with streaming backups via
    ## WAL-E (https://github.com/wal-e/wal-e)
    # databasename-db:
      # waleBackup:
        ## The method to use for backup - either set to WAL (write-ahead-log) or leave blank
        ## for no backups
        # method: WAL

        ## The time (in the server's timezone) to do a full backup of the system
        # executionTime: 03:00

        ## Whether to only read backups but not write them
        # readOnly: false

        ## The WAL-E recovery mode to use - generally leave this out, or set to "immediate" for
        ## the quickest possible recovery at the expense of some data loss
        # recoveryMode:

        ## Settings for pushing backups to Google Storage
        ## The prefix of the place to push the backups
        # gsPrefix: "gs://magda-postgres-backups-asia/dev"

        ## Credentials for authenticating
        # googleApplicationCreds:
        ## The secret to look for the credentials json file in
        #   secretName: storage-account-credentials
        ## The filename of the credentials json mounted in the secret
        #   fileName: TerriaJS-5e042b649f8a.json

    ## Settings for the apidocs server, which serves the documentation for the api from
    ## /api/v0/apidocs/index.html
    apidoc-server:
      replicas: 1
      resources:
        requests:
          cpu: 10m
          memory: 10Mi
        limits:
          cpu: 20m

    ## Settings for the authorization-api, which determines what each user's permissions are:
    authorization-api:
      replicas: 2
      resources:
        requests:
          cpu: 100m
          memory: 75Mi
        limits:
          cpu: 300m

      ## Configure the autoscaler
      autoscaler:
        ## whether to enable autoscaling
        enabled: true
        ## The minimum number of replicas to have
        minReplicas: 2
        ## The maximum number of replicas to have
        maxReplicas: 5


    ## Settings for the authorization-db, which serves the authorization-api
    # authorization-db:

    ## A pod for proxying to Google Cloud SQL, the GCE managed database service.
    cloud-sql-proxy:
      replicas: 2

      resources:
        requests:
          cpu: 150m
          memory: 20Mi
        limits:
          cpu: 300m

      ## The cloud sql instance to connect to
      instanceConnectionName: terriajs:australia-southeast1:magda-prod

    ## A pod that combines all the databases if global.useCombinedDb is set to true
    # combined-db:

    ## A pod that controls the content in magda's UI
    content-api:
      replicas: 2
      
      resources:
        requests:
          cpu: 100m
          memory: 75Mi
        limits:
          cpu: 200m

      ## Configure the autoscaler
      autoscaler:
        ## whether to enable autoscaling
        enabled: true
        ## The minimum number of replicas to have
        minReplicas: 2
        ## The maximum number of replicas to have
        maxReplicas: 5

      ## Override the SCSS variables used by Magda's SCSS files. Look at the "_variables.scss"
      ## file in the magda code for details. Most customisation can be performed by setting
      ## magda-color-primary and magda-color-secondary
      # scssVars: '{"magda-color-primary": "#272727", "magda-color-secondary": "#726a5c"}'

    ## The database behind content-api
    # content-db:

    ## A service that renders emails and forwards them to an SMTP server
    correspondence-api:
      replicas: 2

      ## The default recipient to email when there's no other choice - e.g. when asking a
      ## question about a dataset for which there's no organisational email
      defaultRecipient: "data@digital.gov.au"

      ## The hostname of the SMTP server to use - the password should be in a secret
      smtpHostname: "smtp.mailgun.org"

      ## The SMTP port to use - beware, default SMTP ports are blocked on most cloud
      ## providers
      smtpPort: 2525

      resources:
        requests:
          cpu: 10m
          memory: 60Mi
        limits:
          cpu: 50m

    ## The elasticsearch setup - this can consist of up to three different kinds of pods -
    ## data nodes, master nodes and client nodes.
    elasticsearch:
      ## Whether to use client and master nodes, or just use data nodes.
      production: true

      ## All three kinds of nodes can be customised with these settings like so:
      # nodetype:
        ## The java heap size - this should be half the total memory request
        # heapSize: 3000m
        ## Elasticsearch plugins to install
        # pluginsInstall: "repository-gcs"

      ## Settings for the data nodes - this is a statefulset that actually holds the data,
      ## and is the only kind of node that's strictly necessary. You can have multiple
      ## replicas of these, they will discover each other and balance shards and replicas
      ## between them.
      data:
        heapSize: 3000m
        storage: 200Gi
        # pluginsInstall: "repository-gcs" disabled until es backups are readded
        storageClass: "fast-ssd"
        resources:
          requests:
            cpu: 500m
            memory: 5000Mi
          limits:
            cpu: 1900m

      ## Settings for the client nodes - this accepts HTTP connections from search-api and
      ## indexer and forwards them to the data nodes.
      client:
        replicas: 2
        heapSize: 900m
        # pluginsInstall: "repository-gcs" disabled until es backups are readded
        resources:
          requests:
            cpu: 50m
            memory: 1500Mi
          limits:
            cpu: 250m

      ## Settings for the master nodes - these keep track of the data nodes as they go up
      ## and down and keep track of what the current state of the data should be.
      master:
        # pluginsInstall: "repository-gcs" disabled until es backups are readded
        resources:
          requests:
            cpu: 25m
            memory: 1000Mi
          limits:
            cpu: 250m

    ## The gateway accepts incoming connections and directs them to the appropriate api pod,
    ## or the web server pod
    gateway:
      # service:
        ## The type of kubernetes service to use for the gateway - set to LoadBalancer to
        ## expose an external IP, or set to "NodePort" if using an ingress
        ## Will be overwritten to `NodePort` if deployed through `terraform`
        # type: LoadBalancer

        ## If using a LoadBalancer, the IP to try to use. Leave unset to get an automatic one
        # loadBalancerIP:

      ## If replacing a CKAN installation with Magda, this will redirect CKAN paths without
      ## direct magda equivalents (e.g. the API) to a ckan installation at this domain
      ckanRedirectionDomain: "data.gov.au"
      ckanRedirectionPath: "/data"
      enableCkanRedirection: true

      replicas: 2

      ## Configure the autoscaler
      autoscaler:
        ## whether to enable autoscaling
        enabled: true
        ## The minimum number of replicas to have
        minReplicas: 2
        ## The maximum number of replicas to have
        maxReplicas: 5

      ## Client ids to use for OAuth authentication
      auth:
        ## Client id for facebook
        # facebookClientId: ""
        ## Client id for google
        # googleClientId: ""
        ckanAuthenticationUrl: https://data.gov.au/data
        
        ## If you want to use the Australian Department of Industry's VANguard service, put the details here:
        # vanguardWsFedIdpUrl: https://authentication.business.gov.au/fas/v2/wsfed12/authenticate
        # vanguardWsFedRealm: your-realm-here

      ## Configures the Content Security Policy that will be set on all responses -
      ## the contents will be converted to JSON and passed into https://github.com/helmetjs/csp
      csp:
        directives:
          scriptSrc:
          - "''self''"
          - "''unsafe-inline''" # still being used by rollbar
          - browser-update.org
          - platform.twitter.com
          - www.googletagmanager.com
          - www.google-analytics.com
          - rum-static.pingdom.net
          - https://tagmanager.google.com/debug
          - assets.zendesk.com
          - static.zdassets.com
          - ekr.zdassets.com
          - browser.sentry-cdn.com
          - sentry.cloud.gov.au
          objectSrc:
          - "''none''"
          reportUri: "https://sentry.cloud.gov.au/api/3/security/?sentry_key=af6f917c068e4756837fca7c2f5adf17e"

      ## Configures helmet.js - config will be converted to JSON and passed to
      ## https://github.com/helmetjs/helmet
      helmet:
        hsts:
          maxAge: 31536000
          includeSubdomains: false
          preload: true

      ## Configures cross-origin resource sharing - will be converted to JSON and passed
      ## to https://www.npmjs.com/package/cors#configuring-cors
      cors:
        origin:
        - https://data.gov.au
        - https://search.data.gov.au
        - https://nationalmap.gov.au
        
      ## Enables authorization, include a list of authorization options at /auth/providers
      ## and a simple form at /auth that allows you to test login and logout functionality
      ## without needing a web pod
      enableAuthEndpoint: true

      ## Redirect HTTP urls to HTTPs urls
      enableHttpsRedirection: true

      resources:
        requests:
          cpu: 200m
          memory: 75Mi
        limits:
          cpu: 300m

      ## Turn on HTTP basic authentication - this uses secrets created by the create-secret script
      # enableWebAccessControl: false

    ## Configures the service that puts datasets and organisations into elasticsearch
    indexer:
      elasticsearch:
        ## How many shards and replicas to use when creating a new index. Generally, more
        ## shards mean that the index will be split among more data nodes, and more replicas
        ## allows elasticsearch to copy those shards among different nodes.
        ## In general you want a number of shards equal to the lowest number of data nodes
        ## possible, and a number of replicas equal to the highest number.
        shards: 2
        replicas: 4
      resources:
        requests:
          cpu: 100m
          memory: 500Mi
        limits:
          cpu: 250m

    ## Configures the TerriaJS server responsible for serving preview maps.
    preview-map:
      replicas: 2
      resources:
        requests:
          cpu: 100m
          memory: 750Mi
        limits:
          cpu: 300m

      ## Configure the autoscaler
      autoscaler:
        ## whether to enable autoscaling
        enabled: true
        ## The minimum number of replicas to have
        minReplicas: 2
        ## The maximum number of replicas to have
        maxReplicas: 5

    ## Configures the registry, which holds information about datasets, distributions,
    ## organisations etc and distributes it.
    registry-api:
      # image:
      #   tag: 0.0.55-RC1
      validateJsonSchema: false

      ## Log level to use
      logLevel: INFO

      ## Stops checking for authorization on methods that change the state of the registry.
      ## Useful when running locally - DO NOT TURN ON IN PRODUCTION
      # skipAuthorization: false

      ## Configures the liveness probe that checks if the registry is still alive. This
      ## is configurable seperately because currently the registry api has a tendency to
      ## slow down over time, and needs to be restarted when this occurs.
      livenessProbe:
        timeoutSeconds: 5

      ## Configure the autoscaler
      autoscaler:
        ## whether to enable autoscaling
        enabled: true
        ## The minimum number of replicas to have
        minReplicas: 2
        ## The maximum number of replicas to have
        maxReplicas: 5

      ## The registry api is able to be split into two kinds of deployments: full, which does everything
      ## but can only ever run one pod at a time, and read-only, which can only do read-only requests but
      ## is stateless and can be horizontally scaled
      deployments:
        full:
          # Resources for the full deployment, as per other resource declarations
          resources:
            requests:
              cpu: 300m
              memory: 1250Mi
            limits:
              cpu: 2000m

        readOnly:
          ## Whether to enable the read-only deployment - if not all traffic will go to the full deployment,
          ## otherwise traffic through /api/v0/registry will come here and only traffic to /api/v0/registry-auth
          ## will go to the full deployment
          enable: true
          
          ## Number of replicas
          replicas: 2

          resources:
            requests:
              cpu: 200m
              memory: 300Mi
            limits:
              cpu: 500m


      ## The registry api is able to be split into two kinds of deployments: full, which does everything
      ## but can only ever run one pod at a time, and read-only, which can only do read-only requests but
      ## is stateless and can be horizontally scaled
      # deployments:
        # full:
          ## Resources for the full deployment, as per other resource declarations
          # resources:

        # readOnly:
          ## Whether to enable the read-only deployment - if not all traffic will go to the full deployment,
          ## otherwise traffic through /api/v0/registry will come here and only traffic to /api/v0/registry-auth
          ## will go to the full deployment
          # enable: true
          
          ## Number of replicas
          # replicas: 2

          ## Resources for the full deployment, as per other resource declarations
          # resources:

    ## Configures the registry database, if a separate one is being used.
    # registry-db:

    ## Configures the API that connects to ElasticSearch to perform searches.
    search-api:
      # image:
      #   tag: 0.0.55-RC1
      replicas: 2
      resources:
        requests:
          cpu: 200m
          memory: 600Mi
        limits:
          cpu: 500m

      ## Configure the autoscaler
      autoscaler:
        ## whether to enable autoscaling
        enabled: true
        ## The minimum number of replicas to have
        minReplicas: 2
        ## The maximum number of replicas to have
        maxReplicas: 5

    ## Configures the database used to store session information by passport js, if
    ## one is being used.
    # session-db:

    ## The server used for serving the web application to users.
    web-server:
      replicas: 2
      ## Autoscaler settings - the min and max number of pods to use
      autoscaler:
        enabled: true
        minReplicas: 2
        maxReplicas: 5

      featureFlags:
        cataloguing: false
        publishToDga: false
        placeholderWorkflowsOn: false
        datasetApprovalWorkflowOn: true
        useStorageApi: false

      ## Whether to disable the ability to log in in the UI - set this to true if
      ## you don't want to use any account features.
      disableAuthenticationFeatures: true

      ## If you're replacing an existing system, you can set its url here to display a
      ## banner allowing people to go back if they don't like the new system.
      fallbackUrl: "https://data.gov.au/data/"

      gapiIds:
        - "UA-92539508-1"
        - "UA-38578922-1"

      resources:
        requests:
          cpu: 50m
          memory: 50Mi
        limits:
          cpu: 100m

      # Google Analytics Ids to collect analytics data from the frontend
      # gapiIds:
        # - "ABC123"

      ## The search score after which to show a "suggest a dataset" form
      # datasetSearchSuggestionScoreThreshold: 65

      ## The id of the organization to display by default in the Add Dataset flow.
      # defaultOrganizationId: "abc"
      


    # Settings for the admin API, which allows for changes to be made to the magda
    # installation at runtime... e.g. creating new connector jobs
    admin-api:
      # We only need one of these seeing how it's only admin facing
      replicas: 1
      
      resources:
        requests:
          cpu: 10m
          memory: 50Mi
        limits:
          cpu: 50m

    opa:
      replicas: 2

      resources:
        requests:
          cpu: 200m
          memory: 50Mi
        limits:
          cpu: 500m

      ## Configure the autoscaler
      autoscaler:
        ## whether to enable autoscaling
        enabled: true
        ## The minimum number of replicas to have
        minReplicas: 2
        ## The maximum number of replicas to have
        maxReplicas: 5
